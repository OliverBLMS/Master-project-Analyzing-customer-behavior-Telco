---
title: 'R notebook: Analysing customer behavior in Telco'
author: "Oliver Belmans"
output:
html_notebook: 
toc: yes
html_document: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

# Work plan

Weekly progress is documented in this table. Hard **deadline** is 17-04-2017.

| week | Topic covered |
|------|---------------|
| w1   | Data exploration, pre-processing, features extraction |
| w2   | Create network features |

Things to cover in next weeks:

* Document KDD framework
* network features
* Cluster analysis (quality neuros / cluster) + sequence mining
* Evaluate results,  interpretation
* Documentation + report


# Part 1: Exploration of CDR, pre-processing, making usage features

## Workdirectory and packages/libraries

Basic steps to set the workdirectory and load the necessary packages/libraries.

```{r, message=FALSE, warning=FALSE, include=FALSE}
# get / set workdirectory to read in the datafile
################################################################################################################
# setwd(dir = "/Users/oliver.belmans/Data/R_workdirectory/Thesis")
# getwd()

# load packages/libraries used in the analysis
################################################################################################################
pkg <- c("tidyverse", "igraph", "data.table", "dtplyr", "broom", "purrr", "ggplot2")
# sapply(pkg, install.packages)
sapply(pkg, require, character.only = TRUE )

# # Setting up the parallel computing environment => system.time =:> seems slower on my mac
# # https://www.r-bloggers.com/how-to-go-parallel-in-r-basics-tips/
# ################################################################################################################
# no_cores <- detectCores() - 1
# # Initiate cluster
# cl <- makeCluster(no_cores)
# stopCluster(cl)
```

## Read in data (or make dummy dataset)

The test CDR dataset only contain 100 record, whereas each record represents an edge between two nodes. However, the test set only contains isolated relationships between two nodes. So, instead of working on this test dataset, I created a dummy set on my own. 

```{r}

# read sample dataset (for now make sample set)
################################################################################################################
# read test CDR dataset
read_data <- function(pathfile) {
  temp <- fread(input = pathfile, header = TRUE, sep = ",", stringsAsFactors = FALSE)
  temp$A_NUMBER <- as.character(temp$A_NUMBER)
  temp$B_NUMBER <- as.character(temp$B_NUMBER)
  return(temp)
}
cdr1 <- read_data("PREPAID_CDR_ANONYM_201005.txt")
cdr2 <- read_data("PREPAID_CDR_ANONYM_201006.txt")
cdr3 <- read_data("PREPAID_CDR_ANONYM_201007.txt")


```

## Pre-process 

Responds to following questions:
* Removal of noise or outliers.
* Collecting necessary information to model or account for noise.
* Strategies for handling missing data fields.
*Accounting for time sequence information and known changes.

### Collecting dataset information

Our data contains of only few columns and dimension is of size:
```{r}
# 
cdr1[1:5,1:6]
```

* A_NUMBER: identifier of caller
* B_NUMBER: identifier of callee
* CALL_START_DT: date when call took place
* CALL_START_TM: time when call took place
* CALL_ACTUAL_DURATION is expressed in seconds.
* TARGER_AREA: indicates if destiny of the call is in the providers network


### Remove noise or outliers.

#### Detect outliers based on call duration

What does our dataset looks like, bases on call durations. Use descriptive statistics such as quantiles, mean, etc. to take a first look:
* conclusion: a wide range, with a high maximum.
* Power law distributions: is very skewed, with a long right tail.
* Excluding outliers based on mean +- 3x st. dev, or 1.5* IQR would expect a normal distribution.
* Therefor, try log on the individual columns or scale?

> todo: exlude outliers on log or not? i find  extreme upper point normal (1 hour calls)

```{r}
# summary of call duration
tidy(summary(cdr1$CALL_ACTUAL_DURATION))
# check with visual
hist(cdr1$CALL_ACTUAL_DURATION, prob=TRUE, main="Original distribution" , col=3, xlab = "call duration in seconds")
# check with log histogram, more normal but there is a pike with short call times
hist(log(cdr1$CALL_ACTUAL_DURATION), prob=TRUE, main="Log call duration distribution" , col=4, xlab = "Log(call duration in seconds)")
plot(density(log(cdr1$CALL_ACTUAL_DURATION)))
plot(density((cdr1$CALL_ACTUAL_DURATION)))

```

More in-depth focus on call duration (power law distribution)

```{r}
# long tail, so check the tail
# get boxplots stats: containing the extreme of the lower whisker, the lower ‘hinge’, the median, the upper ‘hinge’ and the extreme of the upper whisker.
stats_boxplot <- boxplot.stats(cdr1$CALL_ACTUAL_DURATION)$stats
stats_boxplot

# density plot on breaks
filter(cdr1, CALL_ACTUAL_DURATION <= 20) %>%
  ggplot(aes(x = CALL_ACTUAL_DURATION )) +
  geom_histogram(breaks = seq(0, 20, 1))

```


* Valid observations: the extreme call durations are valid in my opinion. Calls of duration 3600 seconds are just 1 hour calls. That is quite normal.
* Invalid observations: call with duration between 1-7 seconds are abnormal.  Calls with duration less or equal than 7 seconds are assumed to have been accidentally dropped and are ignored.

#### Excluded data with a call duration threshold

Here, look at how many data will be thrown away if some threshold is applied.

```{r}
# Filter CDR set
########################################################################################
# Outliers are considered as datapoints which are not within 1.5 x IQR
total <- nrow(cdr1)

excluded <- data.frame(call_time = NA,   excluded = NA,  total = NA,  perc_excluded = NA)
summary_set <- NULL
cumsum_exclude = 0

for(call_time  in 1:10){
  include <- nrow(cdr1[cdr1$CALL_ACTUAL_DURATION > call_time])
  exclude <- (total - include) / total *100
  excluded <- rbind(na.omit(excluded), data.frame(call_time, include, total, exclude))
  summary_set <- rbind(summary_set, broom::tidy(summary(cdr1$CALL_ACTUAL_DURATION[cdr1$CALL_ACTUAL_DURATION > call_time])))
}

par(mfrow=c(2,1))
plot(summary_set$median, ylab = "median call duration",  xlab = "minimal threshold of call duration (seconds)")
plot(excluded$exclude, ylab = "data excluded (%)", xlab = "minimal threshold of dall duration (seconds)")
par(mfrow = c(1,1))

# summary table of call duration excluding based on call duration from 1 to 10
summary_set
excluded
```

This table (above) give insights how much data will be thrown away when a certain threshold of minimal of call duration is applied.
* minimal call time above 5 seconds implies that 21,9 % of the original data is thrown away as invalid observations.
* minimal call time above 7 seconds implies that 25.3 % of the original data is thrown away as invalid observations.


```{r}
# drop calls under 5 seconds -> this implies wrong calls or perhaps voicemail calls
filter_sec <- function(dataset) {
  temp <-
    filter(dataset, CALL_ACTUAL_DURATION > 5)
  return(temp)
}

cdr1 <- filter_sec(cdr1)
cdr2 <- filter_sec(cdr2)
cdr3 <- filter_sec(cdr3)

```

#### Detect outliers based on calls (incoming or outgoing calls)

* During pre-processing of the CDR data, excluded the service numbers:
* one way to detect them is look at incoming calls, and exclude the +3 stan dev from the mean number of incoming calls

```{r}

filter_callers <- function(dataset) {
  # calculate number of incoming calls for a month per B_NUMBER
  incoming_calls <-
    dataset %>%
    group_by(B_NUMBER) %>%
    summarise(nr = n()) %>%
    filter(nr > 3000)
  
  # calculate number of incoming calls for a month per A_NUMBER
  outgoing_calls <-
    dataset %>%
    group_by(A_NUMBER) %>%
    summarise(nr = n()) %>%
    filter(nr > 3000)
  
  temp <- 
    filter(dataset, 
           !B_NUMBER %in% c(incoming_calls$B_NUMBER, outgoing_calls$A_NUMBER), 
           !A_NUMBER %in% c(incoming_calls$B_NUMBER, outgoing_calls$A_NUMBER)
    )
  return(temp)
}

cdr1 <- filter_callers(cdr1)
cdr2 <- filter_callers(cdr2)
cdr3 <- filter_callers(cdr3)


# 
# # calculate number of incoming calls for a month per B_NUMBER
# incoming_calls <-
#   cdr1 %>%
#   group_by(B_NUMBER) %>%
#   summarise(nr = n()) %>%
#   arrange(desc(nr))
# 
# # calculate number of incoming calls for a month per A_NUMBER
# outgoing_call <-
#   cdr1 %>%
#   group_by(A_NUMBER) %>%
#   summarise(nr = n()) %>%
#   arrange(desc(nr))

# INCOMING CALL
# head(incoming_calls)
# # get summary stats
# summary(incoming_calls$nr)
# plot(density(incoming_calls$nr))
# plot(density(log(incoming_calls$nr)))
# 
# # get mean
# mean(incoming_calls$nr)
# # get standard dev
# sd(incoming_calls$nr)
# # calculate tripple sigma
# sigma3 <- ceiling(mean(incoming_calls$nr) + 3*sd(incoming_calls$nr))
# sigma
# # Q3 + interquartile *1.5
# outlier_quantile <- boxplot.stats(incoming_calls$nr)$out
# summary(outlier_quantile)
# # only 1 call made: valid or invalid?
# hist(incoming_calls$nr[incoming_calls$nr <= sigma3])
# hist(outlier_quantile)
# plot(x = 1:100, y = head(incoming_calls$nr, 100), type = "b")
# head(incoming_calls)
# # exclude 3671731
# 
# # OUTGOING CALLS
# plot(x = 1:100, y = head(outgoing_call$nr, 100), typ ="b")
# summary(outgoing_call$nr)
# summary(outgoing_call$unique_out)
# head(outgoing_call)

```

* Outgoing call: valid observations, order or max 40 made calls per month is still normal for highly usage.
* Incoming calls: one particual invalid observations of 637 incoming calls (in one month). That is really extreme. 


### Removing customers who call themself

```{r}
filter_self <- function(dataset) {
  temp <-
    filter(dataset, A_NUMBER != B_NUMBER)
  return(temp)
}

cdr1 <- filter_self(cdr1)
cdr2 <- filter_self(cdr2)
cdr3 <- filter_self(cdr3)

```


## Creating breakdown structure

Next step is solving the date format, since the usage features will need a breakdown structure for calculations (based on days, houres, peak houres,..)


```{r}
# Variable manipulations
################################################################################################################

## CALL_START_DT to date format
## Create extra date/time variables like week-, daynumber, houres
breakdown <- function(dataset) {
  temp <-
    dataset %>% 
    mutate(
      CALL_START_DT = as.POSIXct(CALL_START_DT, "%d%b%Y", tz = "GMT"),
      CALL_START_DT_TM = as.POSIXct(paste(CALL_START_DT, CALL_START_TM),"%Y-%m-%d %H:%M:%S", tz = "GMT"),
      WEEK_NR = format(CALL_START_DT_TM, "%W"),
      DAY_NR = format(CALL_START_DT_TM, "%u"),
      DAY = format(CALL_START_DT_TM, "%a"),
      HOUR = as.integer(format(CALL_START_DT_TM, "%H")),
      PEAK_HOUR = ifelse(HOUR >= 9 & HOUR <= 19, "PEAK", "NON_PEAK"),
      WEEKDAYS = ifelse(DAY_NR %in% c(6,7), "WEEKEND", "WEEK")
    )
  return(temp)
}


# breakdown(cdr1 %>% head)

cdr1 <- breakdown(cdr1)
cdr2 <- breakdown(cdr2)
cdr3 <- breakdown(cdr3)


hist(cdr1_breakdown$HOUR)
abline(v = "9", col = "red")
abline(v = "19", col = "red")
```

First, explore our dataset and look for incomplete week sets.

* How many observations do we have on weekly bases
* conclusion: we should only take complete weeks into account

```{r}
# frequency
tidy(table(cdr1_breakdown$WEEK_NR)) %>% 
  ggplot(., aes(x = Var1, y = Freq/1000)) + geom_bar(stat = "identity") + ylab("number of observations (k)")
# first week 17 is not complete
```

```{r}
# Create a sequence interval of the time ordered data
# I choose grouping of 2 weeks
# week 18 - 19 => togehter in the same group nr 18
# week 20 - 21 => together in the same group nr 20
breakdown_groups <- function(dataset) {
  temp <- 
    dataset %>% 
    mutate(
      mygroup = floor(as.numeric(WEEK_NR) / 2) *2) %>% # group 2 weeks in one
    group_by(mygroup) %>%
    # add max date of the 2 week intervalt
    mutate(
      max_date = max(CALL_START_DT)
    ) %>% ungroup 
  return(temp)
}


cdr1 <- breakdown_groups(cdr1)
cdr2 <- breakdown_groups(cdr2)
cdr3 <- breakdown_groups(cdr3)



```


## Bind all subsets together

```{r}
# take full set
cdr_all <- bind_rows(cdr1, cdr2, cdr3)

# dim(cdr1_breakdown)
# take sample data
# do this with the smalles granularity, thus sample % on hour.

# take sample
# cdr_all <- sample_frac(
#   group_by(cdr1_breakdown, HOUR), size = 0.05)
# check dimension
# dim(cdr_all)

```

## Feature engineering

Main idea is to generate usage features for customers in the dataset. However, this needs to be done at regular intervals, taking a sequence static observations at multiple time points and using these intervals for feature extraction. I choose to do this per week, so change of behavior can be tracked by week. I will slice the data per week and create aggregated records of subscribers who where active during that week. So a customer who's active every week will contains +- 16 aggregated records (final dataset contains 4 months, each month +- 4 weeks).

I choose to do feature extraction for these features:

* nr_call = number of calls
* ave_call_time = average call time
* total_call_time = sum of all call times

#### Adding additional features

This steps generates features of the usage of a customer such as number of calls during the 2 week interval, but also more in detail like per day, per (non) peak hours, week or weekend days. Also, for the same breakdown structure the duration of calls is aggregated.


```{r}
# Variable creation based on usage
# create usage KPI 
################################################################################################################
# Create function to calculate the usage features
usage_stats <- function(dataset, group1, group2, ...) {
  # return a dataset with usage features
  dataset %>%
    group_by_(group1, group2, ...) %>%
    summarise(
      nr_call= n(),
      nr_unique_calls = length(unique(B_NUMBER)),
      ave_call_time = mean(CALL_ACTUAL_DURATION),
      total_call_time = sum(CALL_ACTUAL_DURATION)
    ) %>%
    gather_(key_col = "variable", value_col = "value", 
            gather_cols = c(
              "nr_call", 
              "nr_unique_calls",
              "ave_call_time", 
              "total_call_time")) %>%
    unite_("temp", c("variable", ...)) %>%
    spread_("temp", "value", fill = 0) %>%
    ungroup()
}

# number of outbound calls per group = 2 weeks 
# average outbound call duration per group
# total call time per group
week_set <-
  usage_stats(dataset = cdr_all, "mygroup", "A_NUMBER")
# number of outbound calls per day over 2 weeks
# average outbound call duration per day over 2 weeks
# total call time per day over 2 weeks
# day_set <- 
# usage_stats(dataset = cdr_all, "mygroup", "A_NUMBER", "DAY")
# number of outbound calls in (non) peak hours 
# average outbound call duration in (non) peak hours 
# total call time in (non) peak hours 
peak_set <-
  usage_stats(dataset = cdr_all, "mygroup", "A_NUMBER", "PEAK_HOUR")
# number and % of outbound calls in week (end) days
# average outbound call duration per week (end) days
# total call time per week (end) days
weekday_set <- 
  usage_stats(dataset = cdr_all, "mygroup", "A_NUMBER", "WEEKDAYS")
# number and % of outbound calls per target network
# average outbound call duration per target network
# total call time per per target network
network_set <- 
  usage_stats(dataset = cdr_all, "mygroup", "A_NUMBER", "TARGET_AREA")

```


#### Add recency variable

Take into account the recency of call made within the interval of 2 weeks. This is implementen as 1 feature: the latest activity, but also incorporated into the social network analyses as edge weights. 

```{r}
# Calculate recency of calls for input in the decay function

decay_factor <- function(days, factor) {
  exp(1)^(-days * factor)
}

# First is almost linear,
# I prefer factor 0.4
par(mfcol=c(2,2))
plot(x = 0:13, y =  do.call(decay_factor, args = list(0:13, 0.1)), type = "b", xlab = "days", ylab = "recency", main = "exp(1)^(-days*0.1)")
plot(x = 0:13, y =  do.call(decay_factor, args = list(0:13 ,0.2)), type = "b", xlab = "days", ylab = "recency", main = "exp(1)^(-days*0.2)")
plot(x = 0:13, y =  do.call(decay_factor, args = list(0:13, 0.3)), type = "b", xlab = "days", ylab = "recency", main = "exp(1)^(-days*0.3)")
plot(x = 0:13, y =  do.call(decay_factor, args = list(0:13, 0.4)), type = "b", xlab = "days", ylab = "recency", main = "exp(1)^(-days*0.4)")

```

Calculate the recency or activity of each individual call
* used as edge weight in social network analysis

```{r}

# Add recency PER each call based on the 2 week interval
cdr_all <-
  cdr_all %>%
  mutate(
    diffday = as.integer(difftime(max_date,CALL_START_DT, units = "days")),
    weight = decay_factor(diffday, 0.3))
```

* used to calculate latest activity of a caller and added as customer variable

```{r}
# Calculate recency of last activity for a customer PER mygroup 
# get minimum of diffday for each customer
recency_set <- 
  cdr_all %>%
  group_by(mygroup, A_NUMBER) %>%
  summarise(
    # diffday_min = min(diffday),
    recency_min = max(weight)) %>%
  ungroup %>% unique
```


Merge those sets to one dataset containing all the call variables.

```{r}
gc()
# merge dataset to a wide format
dataset_usage_kpi <- 
  Reduce(left_join, list(week_set, peak_set, weekday_set, network_set, recency_set)) 

rm(week_set, peak_set, weekday_set, network_set, recency_set)
gc()
```

## Final dataset with the usage features

Add percentages to all the call nr and call time variables

```{r}

# add percentage features for number of call per X and total call time per X
dataset_usage_kpi <-
  dataset_usage_kpi %>%
  mutate_at(vars(starts_with("nr_call_")), funs("perc" = . / nr_call)) %>%
  mutate_at(vars(starts_with("total_call_time_")), funs("perc" = . / total_call_time)) 
# %>%
# mutate_at(vars(starts_with("nr_unique_")), funs("perc" = . / nr_unique))

# print head full dataset with usage features
head(dataset_usage_kpi)
dim(dataset_usage_kpi)

```

## Add RFM variables

```{r}

dataset_usage_kpi <- 
  dataset_usage_kpi %>%
  group_by(mygroup) %>%
  mutate(global_nr_calls = sum(nr_call),
         global_time_call = sum(total_call_time)) %>% 
  ungroup() %>%
  mutate(
    frequency_perc = nr_call / global_nr_calls,
    mon_value_perc = total_call_time / global_time_call) %>%
  select(-global_nr_calls, -global_time_call) %>%
  ungroup()


```



# Part 2: Social network Analysis

## Creating network
```{r, warning=FALSE}
library(doParallel)  
no_cores <- detectCores() - 1  
cl <- makeCluster(no_cores)  
# registerDoParallel(cl)  # only when using foreach

graphs_per_group <- 
  parLapply(cl, 
            split(
              select(cdr_all, A_NUMBER, B_NUMBER, mygroup),
              cdr_all$mygroup),
            graph_from_data_frame,  vertices = NULL, directed = TRUE )
stopCluster(cl)

# graphs_per_group <- 
#   lapply(
#     split(
#       select(cdr_all, A_NUMBER, B_NUMBER, mygroup),
#       cdr_all$mygroup), 
#     graph_from_data_frame, vertices = NULL,
#     directed = TRUE
#   )

summary(graphs_per_group)

# Creating unique list of vertices 
vertices <-
  unique(select(cdr_all, mygroup, A_NUMBER))
vertices$mygroup <- as.character(vertices$mygroup)
```

## Example plot and network features
+ Decompose the graphs into interconnected graphs
In order to detect noise clusters, we iterate over all clusters and determine their sizes.
Clusters with less than a minimum amount of nodes will have all their nodes excluded. But before this, we need to make sure we don't exclude high profile customer? do we need this

```{r}
g <- simplify(
  graph.compose(
    graph.ring(10),
    graph.star(4, mode = "undirected")
  )
) + edge("7", "8") + edge("2", "3") + edge("6", "1")

plot(g)
clusters(g)
g_decom <- decompose(g)

# no impact from unconnected graphs 
betweenness(g)
lapply(g_decom, betweenness)

# impact from unconnected graphs
closeness(g)
lapply(g_decom, closeness)

# no impact
degree(g)
lapply(g_decom, degree)

# impact
page_rank(g)$vector
lapply(g_decom, page_rank) %>% lapply(., function(x) x$vector)

# no impact
count_triangles(g)
lapply(g_decom, count_triangles)

component_distribution(g, mul.size = TRUE)
is_connected(g)
components(g, mode = "strong")
# ----------------------------------------------------------------------

# # Look if network is fully connected
# network_connected <- lapply(graphs_per_group, is_connected)
# 
# # look at size of each cluster in the network
# # For directed graphs, mode=STRONG 
# # calculates the strongly connected components, mode=WEAK calculates the weakly 
# # connected components (by practically ignoring the edge directions)
cluster_network <-
  lapply(graphs_per_group, components, mode = "weak")

# Which A_NUMBERS are in those clusters
cluster_membership <-
  lapply(cluster_network, function(x) x$membership) %>%
  lapply(as.data.frame) %>%
  lapply(., setNames, nm = "membership") %>%
  lapply(., rownames_to_column,  var = "NUMBER") %>%
  bind_rows(.id = "mygroup") %>%
  group_by(mygroup, membership) %>%
  mutate(csize = n()) %>%
  ungroup()

# summarise cluster size to see how big each cluster is, meaning how many clusters of a size x are there
cluster_size_sum <-
  cluster_membership %>%
  group_by(mygroup, csize) %>%
  summarise(nr_per_cluster = n_distinct(membership)) %>% 
  mutate(total_cust = nr_per_cluster * csize,
         cumsum_total_cust = cumsum(total_cust) / sum(total_cust))
```

Compairing the different subgraphs, for how much % do they represent the data.
```{r}
ggplot(cluster_size_sum, aes( x = log(csize), y = cumsum_total_cust)) +
  geom_point() + geom_line() + 
  # coord_cartesian(xlim = c(0, 10)) +
  # scale_x_continuous(breaks = seq(0,20,1), labels = seq(0,20,1)) +
  facet_wrap(~mygroup, scales = "free")
```


```{r}

# # Look at top of clusters and their size.
# # Enormous spike at cluster size. Lets check this out
top_n(cluster_size_sum, n = 1, wt = csize)

# Take all vertices of the highest connect subgraph
# Take this list to take the subset of customers to calculate the usage dataset
list_top_clusters <-
  cluster_membership %>% 
  select(mygroup, NUMBER,  csize) %>%
  group_by(mygroup) %>%
  filter(csize == max(csize)) %>%
  ungroup()


# TAKE LARGEST SUBGRAPH FUNCTION
giant.component <- function(graph, ...) {
  cl <- components(graph, ...)
  induced_subgraph(graph, which(cl$membership == which.max(cl$csize)))
  
}
# TAKE LARGEST SUBGRAPH
largest_graphs_per_group <- lapply(graphs_per_group, giant.component)

# Check dimension of graph
sapply(largest_graphs_per_group, gsize)

# Make A_NUMBER selection of the largest subgraph
# the NUMBERS of the subgraph are A_ & B_NUMBERS
# therefor, take a innerjoin per group to keep the A_NUMBERS in the largest subgraph

cdr1_breakdown$mygroup <- as.character(cdr1_breakdown$mygroup)

clear_sample <- 
  inner_join(list_top_clusters,cdr1_breakdown, by = c("mygroup" = "mygroup", "NUMBER" = "A_NUMBER"))



```

Are those high monetary value customers, meaning they represent a high volume or frequency of calls?

```{r}


# split largest network from the rest

gg <- induced_subgraph(graphs_per_group[['18']], vids = list_temp)


# check how many incoming edges it has
gg_deg_in_graphs <-
  degree(gg,  mode="in", loops = FALSE, normalized = FALSE) %>%
  as.data.frame() %>%
  rownames_to_column(., var = "B_NUMBER")
gg_deg_out_graphs <-
  degree(gg,  mode="out", loops = FALSE, normalized = FALSE) %>%
  as.data.frame() %>%
  rownames_to_column(., var = "A_NUMBER")

```



## Help function to convert network features into dataframe

todo: probeer simplify te gebruiken om multiple edges om te zetten naar weight

```{r}
return_dataset <- function(list_metrics, var_name)
{
  lapply(list_metrics, as.data.frame) %>%
    lapply(., select, 1) %>%
    lapply(., setNames, nm = var_name) %>% 
    lapply(., rownames_to_column,  var = "A_NUMBER") %>% 
    bind_rows(.id = "mygroup") %>%
    right_join(., vertices)
}

```

## Creating network features

Uitleg van verschillende centrality metrics

Foto's van definites toevoegen
plot degree distribution of network

#### Degree: just number of in & out
doest not factor in distance or weight


```{r}
# Degree (number of ties)
deg_all_graphs <-  
  return_dataset(
    lapply(graphs_per_group, degree, mode = "all", loops = FALSE, normalized = FALSE),
    "degree_all") 
deg_in_graphs <- 
  return_dataset(
    lapply(graphs_per_group, degree, mode="in", loops = FALSE, normalized = FALSE),
    "degree_in")
deg_out_graphs <- 
  return_dataset(
    lapply(graphs_per_group, degree, mode="out", loops = FALSE, normalized = FALSE),
    "degree_out")

# summary(deg_all_graphs)
# check ok

```

#### Strength or weighted vertex degree
* Summing up the edge weights of the adjacent edges for each vertex.
* better estimate for relevance of nodes since high number indicates more recent activities
* todo: normalize it

```{r}
# calculate the "weighted degree": Summing up the edge weights of the adjacent edges for each vertex.
# strenght_all <- lapply(graphs_per_group, strength, mode = "all", loops = FALSE) 
# strenght_in <- lapply(graphs_per_group, strength, mode = "in", loops = FALSE)
# strenght_out <- lapply(graphs_per_group, strength, mode = "out", loops = FALSE)
```


#### Closeness centrality of vertices

```{r}
# -> intensive computing
# closeness: shortest path / nodes, vanuit 1 enkele node naar andere
# farness/peripherality of a node v is defined as the sum of its distances to all other nodes
# Closeness (centrality based on distance to others in the graph)
# Inverse of the node’s average geodesic distance to others in the network.
# The more central a node is, the lower its total distance to all other nodes
net_closeness_in <-   
  return_dataset(
    lapply(graphs_per_group, estimate_closeness, cutoff = 10,  mode = "in", normalized = FALSE),
    "closeness_in")
net_closeness_out <- 
  return_dataset(
    lapply(graphs_per_group, estimate_closeness, cutoff = 10,  mode = "out", normalized = FALSE),
    "closeness_out")


# closeness(graph = net, vids = V(net), mode = "all", normalized = TRUE)
# check ok
```


#### Transitivity of a graph

```{r}
# clustering coefficient.
# P(two randomly selected friends of A are friends)
# Transitivity measures the probability that the adjacent vertices of a vertex are connected. 
# This is sometimes also called the clustering coefficient. (0 = star, 1 clique)
take_transitivity <- function(graph, ...) {
  trans <- transitivity(graph, type = 'localundirected', isolates = 'zero')
  get_names <- V(graph)$name
  data.frame(A_NUMBER = get_names, transitivity = trans)  
}

net_transitivity <- 
  lapply(graphs_per_group, simplify) %>%
  lapply(., take_transitivity) %>%
  bind_rows(.id = "mygroup") %>%
  right_join(., vertices)

#check ok
```

#### Find triangles in graphs

```{r}
# Count how many triangles a vertex is part of, in a graph, or just list the triangles of a graph.
take_triangles <- function(graph, ...) {
  count_tri <- count_triangles(graph, ...)
  get_names <- V(graph)$name
  data.frame(A_NUMBER = get_names, triangles = count_tri)  
}
net_triangles <- 
  lapply(graphs_per_group, take_triangles) %>%
  bind_rows(.id = "mygroup") %>%
  right_join(., vertices)

# check ok
```

#### Vertex and edge betweenness centrality

```{r}
# -> very intensive computing

# Betweenness: defined by the number of geodesics (shortest paths) going through a vertex or an edge.
# Betweenness centrality quantifies the number of times a node acts as a bridge along the shortest path between two other nodes
# Betweenness (centrality based on a broker position connecting others)
# Number of geodesics that pass through the node or the edge.
# if to slow try estimate_betweeness
net_betweenness <- 
  return_dataset(
    lapply(graphs_per_group, simplify) %>%
      lapply(., estimate_betweenness, directed = TRUE, cutoff = 8),
    "betweenness")

# check ok
```


#### Pagerank

```{r}
# need different return_dataset function
# Google’s PageRank is a variant of the Eigenvector centrality measure for directed network

net_pagerank <- 
  lapply(graphs_per_group, page_rank, directed = TRUE, damping = 0.85) %>%
  map(., 1) %>%
  lapply(., as.data.frame) %>%
  lapply(., setNames, nm = "pagerank") %>% 
  lapply(., rownames_to_column,  var = "A_NUMBER") %>% 
  bind_rows(.id = "mygroup") %>%
  right_join(., vertices)


# check ok
```


# Putting it all together

```{r}

dataset_usage_kpi$mygroup <- as.character(dataset_usage_kpi$mygroup)
dataset_usage_kpi$A_NUMBER <- as.character(dataset_usage_kpi$A_NUMBER)

finalset <- 
  Reduce(left_join, 
         list(
           dataset_usage_kpi,
           deg_all_graphs, 
           deg_in_graphs, 
           deg_out_graphs,
           net_betweenness,
           net_closeness_in,
           net_closeness_out,
           net_pagerank,
           net_triangles,
           net_transitivity
         ))

# Backup made from here
# save(finalset, file = "finalset.RData")

# fill NA of transitivity or triangles with 0

finalset[is.na(finalset)] <- 0

rm(deg_all_graphs,deg_in_graphs, deg_out_graphs, net_betweenness, net_closeness_in,
   net_closeness_out, net_pagerank, net_triangles, net_transitivity)

```


# BACKUP STEP: finalset vanaf hier

* Make unique rownmames of mygroup + A_NUMBER
* select business relevant features (apriori knowledgde)
```{r}
# finalset <- sample_frac(finalset, size = 0.30, replace = FALSE )
# Make a rownames of the group + A-NUMBER
finalset_key <- 
  finalset %>%
  unite(col = key, mygroup, A_NUMBER, sep = "_", remove = FALSE)
# 
# class(finalset) <- "data.frame"
# row.names(finalset) <- finalset$key
finalset_key <- finalset_key[,-c(2:3)]

# Remove irrelevant features
# keep everything, except home, other net
clusterset <- 
  dplyr::select(finalset_key,
                key,
                nr_unique_calls,
                recency_min,
                nr_call,
                total_call_time,
                ave_call_time,
                nr_call_NON_PEAK_perc,
                nr_call_PEAK_perc,
                nr_call_WEEK_perc,
                nr_call_WEEKEND_perc,
                total_call_time_NON_PEAK_perc,
                total_call_time_PEAK_perc,
                total_call_time_WEEK_perc,
                total_call_time_WEEKEND_perc,
                degree_all,
                degree_in,
                betweenness,
                closeness_in,
                closeness_out,
                pagerank,
                triangles,
                transitivity)

# TAke scaling: Range vs scaling (x-md/sd)
range01 <- function(x){(x-min(x))/(max(x)-min(x))}
### Range everything between value of [0-1]
# -key, do NOT range the key column
clusterset_ranged <-
  clusterset %>%
  mutate_each(funs((.-min(.))/(max(.)-min(.))), -key)

# Add +1
# Take log from features


clusterset_log <-
  clusterset %>%
  mutate_at(
    vars(ave_call_time, betweenness, closeness_in,
         closeness_out, degree_all, degree_in, nr_call, nr_unique_calls, 
         pagerank, total_call_time, transitivity, triangles),
    funs(.+1)) %>%
  mutate_at(
    vars(ave_call_time, betweenness, closeness_in,
         closeness_out, degree_all, degree_in, nr_call, nr_unique_calls, 
         pagerank, total_call_time, transitivity, triangles),
    funs(log10(.))) %>%
  mutate_each(funs((.-min(.))/(max(.)-min(.))), -key)


### Scale 
# clusterset_scaled <-
#   clusterset %>%
#   mutate_each(funs((. - mean(.)) / sd(.)), -key)

```

### Look at variables distribution
```{r}
clusterset_long <-
  clusterset %>%
  gather(variable, value, -key)

clusterset_ranged_long <-
  clusterset_ranged %>%
  gather(variable, value, -key)

clusterset_log_long <-
  clusterset_log %>%
  gather(variable, value, -key)


ggplot(clusterset_long, aes(x = value)) + geom_histogram(bins = 40) + facet_wrap(facets = "variable", scales = "free", ncol = 4) + theme_bw()
ggplot(clusterset_ranged_long, aes(x = value)) + geom_histogram(bins = 40) + facet_wrap(facets = "variable", scales = "free", ncol = 4) + theme_bw()
ggplot(clusterset_log_long, aes(x = value)) + geom_histogram(bins = 40) + facet_wrap(facets = "variable", scales = "free", ncol = 4) + theme_bw()


```



### Delete correlated features
```{r}
require(caret)
# 
delete_correlation <- function(dataset, threshold, plotcor) {
  require(caret)
  # plot correlation plot
  cor_set <- cor(dataset[,-1])
  if(plotcor == TRUE) corrplot::corrplot(cor_set, tl.cex = 0.5, type = "upper", method = "number", number.cex = 0.5)
  # This function searches through a correlation matrix and returns a vector of integers corresponding to columns to remove to reduce pair-wise correlations.
  high_correlated = findCorrelation(cor_set, cutoff= threshold, exact = TRUE)
  # remove higly correlated
  if(length(high_correlated) > 0){
    cor_set_return = dataset[,-c(high_correlated+1)]
  }
  
  return(cor_set_return)
}
# 
# # clusterset with no correlated features
# # set correlation threshold
# clusterset_no_cor <- delete_correlation(clusterset_ranged, 0.8, FALSE)
clusterset_log_nocor <- delete_correlation(clusterset_log, 0.8, FALSE) # no perc
# clusterset3_no_cor <- delete_correlation(clusterset3_scaled, 0.8, FALSE)

# Manual check for correlation
cor_set <- cor(clusterset_log[,-1])
corrplot::corrplot(cor_set, tl.cex = 0.5, type = "lower", method = "number", number.cex = 0.5)

high_correlated = findCorrelation(cor_set, cutoff= 0.9, exact = TRUE, names = TRUE) 
high_correlated
```


### Manual selection of variables
```{r}
clusterset_log_manual <-
  dplyr::select(
    clusterset_log,
    key,
    nr_unique_calls,
    recency_min,
    nr_call,
    total_call_time,
    ave_call_time,
    nr_call_PEAK_perc,
    nr_call_WEEK_perc,
    total_call_time_PEAK_PEAK_perc,
    total_call_time_WEEK_perc,
    degree_all,
    degree_in,
    betweenness,
    closeness_in,
    pagerank,
    triangles,
    transitivity
  )

```

# Part 3: Cluster

## K-mean attempt

Do this several times with incremented number of clusters.
* Evaluatie by looking at the SSE , this is called the elbow technique
* another approach, take sameples and do the algorithm. Look at elbow plot, evaluate with db, ch metrics (full dataset clustering takes ages :) )

```{r}
kmean_fun <- function(dataset, max_cluster) {
  # clusterSetRNGStream()
  require(foreach)
  require(doParallel)
  no_cores <- detectCores()-3 # go with 4 cores
  #nr of processors available registerDoParallel(cl)
  cl<-makeCluster(no_cores) #nr of processors available 
  registerDoParallel(cl)
  
  # result.kmean <- (nrow(dataset)-1)*sum(apply(dataset,2,var))
  result.kmean <- foreach(i=2:max_cluster) %dopar% {
    km <- kmeans(dataset, centers = i, iter.max = 30, nstart = 10);
    return(km)
  }
  # stop parallel cores
  stopImplicitCluster()
  stopCluster(cl)
  # return list of kmean results
  return(result.kmean)
}


kmean_ranged <- kmean_fun(clusterset_log[,-1], 10)

```

## Kmeans-run (detect optimal clusters automatically)
Try other function: fpc::kmeansruns
* this will also incremented the number of cluster in each run and eavluate it automatically with the Calinski Harabasz index

```{r}
library(fpc)
# run kmeansruns
kmeansrun_ranged <- kmeansruns(clusterset_ranged[,-1], krange = 1:15, criterion = "ch", runs = 10, iter.max = 30)
# nr of cluster based on CH criteria
kmeansrun_ranged$bestk
```

### Elbow technique for Kmean
Plot the withiness sum of squared distance
```{r}
elbow_plot <- function(dataset, plotname) {
  plot(2:10, sapply(dataset, function(x) x$tot.withinss), type="b", xlab="Number of Clusters", ylab="Within groups sum of squares", main = plotname)
}

# show plot
elbow_plot(kmean_ranged, "Clusterset log")
# safe plot
png("elbow_kmeans_log.png", width = 500,  height = 800, res = 100)
par(mfrow=c(1,1))
elbow_plot(kmean_ranged, "Clusterset log")
dev.off()


```

## K-Medoid

PAM (Partitioning Around Medoids) is a classic algorithm for k-medoids clustering. While the PAM algorithm is inefficient for clustering large data, the CLARA algorithm is an enhanced technique of PAM by drawing multiple samples of data, applying PAM on each sample and then returning the best clustering. It performs better than PAM on larger data. Functions pam() and clara() in package cluster [Maechler et al., 2015] are respectively im-plementations of PAM and CLARA in R. For both algorithms, a user has to specify k, the number of clusters to find. As an enhanced version of pam(), function pamk() in package fpc [Hennig, 2015] does not require a user to choose k. Instead, it calls the function pam() or clara() to perform a partitioning around medoids clustering with the number of clusters estimated by optimum average silhouette width.

Algorithm:
* Split randomly the data sets in multiple subsets with fixed size
* Compute PAM algorithm on each subset and choose the corresponding k representative objects (medoids).  Assign each observation of the entire dataset to the nearest medoid.
* Calculate the mean (or the sum) of the dissimilarities of the observations to their closest medoid. This is used as a measure of the goodness of the clustering.
* Retain the sub-dataset for which the mean (or sum) is minimal. A further analysis is carried out on the final partition.

> Very fast with large n

```{r}
require(fpc)
require(cluster)
# Use k-mediod method: pamk function that will return best nr of clusters based on ch
# pamk_ranged <- pamk(clusterset_ranged[,-1], krange = 1:15, criterion = "ch", usepam = FALSE)
# # nr of cluster
# pamk_ranged$nc

# Run clara with a range of clusters
# Evaluate later on the optimal cluster with the evaluation statistic CH, DB
clara_fun <- function(dataset, max_cluster) {
  require(cluster)
  require(foreach)
  require(doParallel)
  no_cores <- detectCores()-6
  #nr of processors available registerDoParallel(cl)
  cl<-makeCluster(no_cores) #nr of processors available
  registerDoParallel(cl)
  
  result.clara <- foreach(i=2:max_cluster,.verbose = TRUE,
                          .packages = 'cluster') %dopar% {
                            pam_set <- clara(dataset, k = i, metric = "euclidean", stand = FALSE, samples = 100)
                            return(pam_set)
                          }
  
  # 
  stopImplicitCluster()
  stopCluster(cl)
  return(result.clara)
}



# run clara function
clara_ranged <- clara_fun(clusterset_log[,-1], 10)

```
### Evalute k-medoid
#### Average silhouette width (based on best random subesets)

Silhouette analysis measures how well an observation is clustered and it estimates the average distance between clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters.

Silhouette width can be interpreted as follow:
* Observations with a large SiSi (almost 1) are very well clustered
* A small SiSi (around 0) means that the observation lies between two clusters
* Observations with a negative SiSi are probably placed in the wrong cluster.

> By default, for clara() partitions, the silhouette is just for the best random subset used. Use full = TRUE to compute (and later possibly plot) the full silhouette :> DONT DO THIS because this will fail for the reason that n is to large to compute pairwise-distance matrix.

* Silhouette statistic is not optimal criteria for cluster validation on this setup (best random subset is used). Instead used the CH, DB cluster validation on the whole dataset.

```{r}
# plot ave width of silhouette info
plot(2:12, sapply(clara_ranged, function(x) x$silinfo$avg.width), type="b", xlab="Number of Clusters", ylab="Average silhouette width", main = "CLARA (k-mediod)")

# GET RESULTS
# k.best <- which.max(sapply(clara_ranged, function(x) x$silinfo$avg.width))
# cat("silhouette-optimal number of clusters:", k.best +1, "\n")
# # get result out of clara_ranged list
# clara_result <- clara_ranged[[k.best]]
```

## Sk means
* can't perform when using 0-1 range.

```{r, eval=FALSE, include=FALSE}
library(skmeans)
# Run clara with a range of clusters
# Evaluate later on the optimal cluster with the evaluation statistic CH, DB
skmeans_fun <- function(dataset, max_cluster) {
  require(foreach)
  require(doParallel)
  no_cores <- detectCores()-4
  #nr of processors available registerDoParallel(cl)
  cl<-makeCluster(no_cores) #nr of processors available
  registerDoParallel(cl)
  
  result.skmeans <- foreach(i= 2:max_cluster, .packages = 'skmeans') %dopar% {
    skmeans_set <- skmeans(x = as.matrix(dataset), k = i, control = list(verbose = TRUE))
    return(skmeans_set)
  }
  # stop parallel computing
  stopImplicitCluster()
  stopCluster(cl)
  # return result as list
  return(result.skmeans)
}

skmeans_ranged <- skmeans_fun(clusterset[,-1], 12)

```

## DBSCAN
```{r, eval=FALSE, include=FALSE}
library(dbscan) # used to get KNN distance plot
library(fpc)    # used dbscan of the fpc package

distplot_clusterset_ranged <- kNNdistplot(clusterset_ranged[,-1], k =  10)
distplot_clusterset_ranged

abline(h = 0.21, lty = 2)

dbscan_clusterset_scaled <- dbscan::dbscan(x = clusterset_ranged[,-1], eps = 0.2, borderPoints = TRUE)
```



## Evaluate clusters
K-means is all about centroids and variance and therefore criterions such as Calinski-Harabasz are more focused here.

Use following functions to calculate Calinski-Harabsz and Davies Boulding.

### Calinski-Harabsz and Davies Boulding cluster quality measures

* function to retrieve CH, DB stats

```{r}
ch_plot <- function(dataset, datacluster, plotname, method) {
  require(clusterSim)
  # get correct cluster results, depens on algorithm used
  if(method == "kmean")
  {
    x_values = 2:10
    ch_values = sapply(datacluster, function(i) index.G1(dataset[,-1], cl = i$cluster))
    db_values = sapply(datacluster, function(i) index.DB(dataset[,-1], cl = i$cluster)$DB)
  } else {
    x_values = 2:10
    ch_values = sapply(datacluster, function(i) index.G1(dataset[,-1], cl = i$clustering))
    db_values = sapply(datacluster, function(i) index.DB(dataset[,-1], cl = i$clustering)$DB)
    
  }
  png(paste0(deparse(substitute(datacluster)),"_log_final.png"), width = 800,  height = 500, res = 100)
  par(mfrow=c(1,2))
  plot(x_values, ch_values, type="b", xlab="Number of Clusters", ylab="Calinski-Harabasz", main = plotname)
  plot(x_values, db_values, type="b", xlab="Number of Clusters", ylab="Davies-Bouldin", main = plotname)
  dev.off()
}

# check fit of clusters
# return single 
measure_cluster <- function(dataset, datacluster) {
  ####STATISTICS
  require(clusterSim)
  cat(deparse(substitute(datacluster))) 
  indexCH <- index.G1(dataset[,-1], datacluster)
  indexDB <- index.DB(dataset[,-1], datacluster) 
  cat('\nDavies-Bouldin',indexDB$DB,'\nCalinski-Harabasz',indexCH,'\n') 
  cat('-------------\n')
  # list(CH = indexCH, DB = indexDB$DB)
}

```

* Get stats for the different cluster methods

```{r}
# Kmeans
# plot function
ch_plot(clusterset_log, kmean_ranged, "Clusterset log kmean", method = "kmean")
ch_plot(clusterset_log, clara_ranged, "Clusterset log clara (Euclidian)", method = "clara")


# Get stats for a specific cluster
sink("db_ch_statistics_log_set.txt")
measure_cluster(clusterset_log, kmean_ranged[[2]]$cluster)
measure_cluster(clusterset_log, kmean_ranged[[3]]$cluster)
measure_cluster(clusterset_log, kmean_ranged[[4]]$cluster)
measure_cluster(clusterset_log, kmean_ranged[[5]]$cluster)
measure_cluster(clusterset_log, kmean_ranged[[6]]$cluster)
measure_cluster(clusterset_log, kmean_ranged[[7]]$cluster)
measure_cluster(clusterset_log, kmean_ranged[[8]]$cluster)
sink()

# Get stats for a specific cluster
sink("db_ch_statistics_clara_log_set.txt")
measure_cluster(clusterset_log, clara_ranged[[2]]$cluster)
measure_cluster(clusterset_log, clara_ranged[[3]]$cluster)
measure_cluster(clusterset_log, clara_ranged[[4]]$cluster)
measure_cluster(clusterset_log, clara_ranged[[5]]$cluster)
measure_cluster(clusterset_log, clara_ranged[[6]]$cluster)
measure_cluster(clusterset_log, clara_ranged[[7]]$cluster)
measure_cluster(clusterset_log, clara_ranged[[8]]$cluster)
sink()

# safe output to console
sink(file = "Cluster stats.txt")
print(kmeansrun_ranged)
print("-----------------------")
print(kmean_ranged[[8]])
print("-----------------------")
print(kmean_ranged[[10]])
print("-----------------------")
print(clara_ranged[[7]])
print("-----------------------")
print(pamk_ranged)
sink()


```


# Describe cluster

## Add clusternumber to observations

```{r}
# Make a dataset of the clusterset (original values) plus the clusternumber from the cluster results
clusterset_final <-
  bind_cols(clusterset, data.frame(cluster =  kmean_ranged[[4]]$cluster))
```

## Cluster interpretation
```{r}
clusterset_pop <-
  clusterset_final %>% dplyr::select(-key) %>%
  gather(variable, value, -cluster) %>%
  group_by(variable) %>%
  summarise(mean_population = mean(value))

clusterset_cluster <-
  clusterset_final %>% dplyr::select(-key) %>%
  gather(variable, value, -cluster) %>%
  group_by(cluster, variable) %>%
  summarise(mean_cluster = mean(value))

clusterset_long <-
  clusterset_final %>% dplyr::select(-key) %>%
  gather(variable, value, -cluster) %>%
  group_by(variable) %>%
  mutate(mean_population = mean(value)) 

# plots
filter(clusterset_long, variable == "recency_min") %>% ggplot(aes(x = value, fill = factor(cluster))) + geom_density(alpha= 0.5, position = "stack")

filter(clusterset_long, variable == "triangles") %>% ggplot(aes(x = value, fill = factor(cluster))) + geom_density(alpha= 0.5, position = "stack")

```


## Make visual plot of cluster characteristics
* this is also scaled
Look at cluster characteristics 
* mean
* var

```{r}

cluster_analyse <- function(dataset, clustervector) {
  nameofcluster <- deparse(substitute(clustervector))
  # characteristics of each cluster
  cluster_mean <- 
    aggregate(clusterset[,-1],  by = list(cluster = clustervector), mean)
  cluster_var <- 
    aggregate(clusterset[,-1],  by = list(cluster = clustervector), var)
  # print to txt
  # out <- capture.output(cluster_mean)
  # cat(out, file= paste0(nameofcluster,".txt"), sep="n")
  # 
  # put it in a long format to plot with ggplot
  cluster_mean_long <-
    cluster_mean %>%
    gather(vars, value, -cluster) %>%
    group_by(vars) %>%
    mutate(value = range01(value))  
  # ggplot heatmap with profile of each cluster
  
  orderit <- 
    c("recency_min", "nr_call", "total_call_time", "ave_call_time", 
      "degree_all", "degree_in", "betweenness", "closeness_in", "closeness_out", 
      "pagerank", "triangles", "transitivity")
  
  cluster_mean_long$vars <- factor(cluster_mean_long$vars, levels = orderit)
  
  ggplot(cluster_mean_long, aes(x = as.factor(cluster), y = vars)) + geom_tile(aes(fill = value), color = "white") + theme_minimal()+ scale_fill_gradient(low = "white", high = "darkblue") + theme_bw() + theme(axis.ticks = element_blank(), panel.border = element_blank()) + scale_x_discrete(expand = c(0, 0))
  ggsave(filename =  paste0(nameofcluster,".png"), device = "png")
  
}

# plot 
cluster_analyse(clusterset, kmean_ranged[[4]]$cluster)
```

# Part 4: Sequence mining 

Next, lets do some sequence mining on the cluster we have for each customer on different time-periods. Before that, there needs to be some trasformztion done.
* get clusterresult on a customer level, this is for each timeperiod the assigned cluster
* add minilal support on the frequent itemset for the sequence mining

```{r}
library(arulesSequences)

pos_seq <- data_frame(key = clusterset_ranged[,1], cluster = clara_ranged[[7]]$clustering)

# Transform data set
pos_seq <- 
  pos_seq %>%
  separate(col = key, into = c("mygroup", "A_NUMBER"), sep = "_" )
```

