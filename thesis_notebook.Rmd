---
title: 'R notebook: Analysing customer behavior in Telco'
author: "Oliver Belmans"
output:
html_notebook: 
toc: yes
html_document: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

# Work plan

Weekly progress is documented in this table. Hard **deadline** is 17-04-2017.

| week | Topic covered |
|------|---------------|
| w1   | Data exploration, pre-processing, features extraction |
| w2   | Create network features |

Things to cover in next weeks:

* Document KDD framework
* network features
* Cluster analysis (quality neuros / cluster) + sequence mining
* Evaluate results,  interpretation
* Documentation + report


# Part 1: Exploration of CDR, pre-processing, making usage features

## Workdirectory and packages/libraries

Basic steps to set the workdirectory and load the necessary packages/libraries.

```{r, message=FALSE, warning=FALSE, include=FALSE}
# get / set workdirectory to read in the datafile
################################################################################################################
setwd(dir = "/Users/oliver.belmans/Data/R_workdirectory/Thesis")
getwd()

# load packages/libraries used in the analysis
################################################################################################################
pkg <- c("tidyverse", "igraph", "data.table", "dtplyr", "broom", "purrr", "ggplot2")
sapply(pkg, require, character.only = TRUE )

# # Setting up the parallel computing environment => system.time =:> seems slower on my mac
# # https://www.r-bloggers.com/how-to-go-parallel-in-r-basics-tips/
# ################################################################################################################
# no_cores <- detectCores() - 1
# # Initiate cluster
# cl <- makeCluster(no_cores)
# stopCluster(cl)
```

## Read in data (or make dummy dataset)

The test CDR dataset only contain 100 record, whereas each record represents an edge between two nodes. However, the test set only contains isolated relationships between two nodes. So, instead of working on this test dataset, I created a dummy set on my own. 

```{r}

# read sample dataset (for now make sample set)
################################################################################################################
# read test CDR dataset
cdr1 <- fread(input = "PREPAID_CDR_ANONYM_201005.txt", header = TRUE, sep = ",", stringsAsFactors = FALSE)
# cdr2 <- fread(input = "PREPAID_CDR_ANONYM_201006.txt", header = TRUE, sep = ",", stringsAsFactors = FALSE)
dim(cdr1)
cdr1$A_NUMBER <- as.character(cdr1$A_NUMBER)
cdr1$B_NUMBER <- as.character(cdr1$B_NUMBER)

```

## Pre-process 

Responds to following questions:
* Removal of noise or outliers.
* Collecting necessary information to model or account for noise.
* Strategies for handling missing data fields.
*Accounting for time sequence information and known changes.

### Collecting dataset information

Our data contains of only few columns and dimension is of size:
```{r}
# 
cdr1[1:5,1:6]
```

* A_NUMBER: identifier of caller
* B_NUMBER: identifier of callee
* CALL_START_DT: date when call took place
* CALL_START_TM: time when call took place
* CALL_ACTUAL_DURATION is expressed in seconds.
* TARGER_AREA: indicates if destiny of the call is in the providers network


### Remove noise or outliers.

#### Detect outliers based on call duration

What does our dataset looks like, bases on call durations. Use descriptive statistics such as quantiles, mean, etc. to take a first look:
* conclusion: a wide range, with a high maximum.
* Power law distributions: is very skewed, with a long right tail.
* Excluding outliers based on mean +- 3x st. dev, or 1.5* IQR would expect a normal distribution.
* Therefor, try log on the individual columns or scale?

> todo: exlude outliers on log or not? i find  extreme upper point normal (1 hour calls)

```{r}
# summary of call duration
tidy(summary(cdr1$CALL_ACTUAL_DURATION))
# check with visual
hist(cdr1$CALL_ACTUAL_DURATION, prob=TRUE, main="Original distribution" , col=3, xlab = "call duration in seconds")
# check with log histogram, more normal but there is a pike with short call times
hist(log(cdr1$CALL_ACTUAL_DURATION), prob=TRUE, main="Log call duration distribution" , col=4, xlab = "Log(call duration in seconds)")
plot(density(log(cdr1$CALL_ACTUAL_DURATION)))
plot(density((cdr1$CALL_ACTUAL_DURATION)))

```

More in-depth focus on call duration (power law distribution)

```{r}
# long tail, so check the tail
# get boxplots stats: containing the extreme of the lower whisker, the lower ‘hinge’, the median, the upper ‘hinge’ and the extreme of the upper whisker.
stats_boxplot <- boxplot.stats(cdr1$CALL_ACTUAL_DURATION)$stats
stats_boxplot

# density plot on breaks
filter(cdr1, CALL_ACTUAL_DURATION <= 20) %>%
  ggplot(aes(x = CALL_ACTUAL_DURATION )) +
  geom_histogram(breaks = seq(0, 20, 1))

```


* Valid observations: the extreme call durations are valid in my opinion. Calls of duration 3600 seconds are just 1 hour calls. That is quite normal.
* Invalid observations: call with duration between 1-7 seconds are abnormal.  Calls with duration less or equal than 7 seconds are assumed to have been accidentally dropped and are ignored.

#### Excluded data with a call duration threshold

Here, look at how many data will be thrown away if some threshold is applied.

```{r}
# Filter CDR set
########################################################################################
# Outliers are considered as datapoints which are not within 1.5 x IQR
total <- nrow(cdr1)

excluded <- data.frame(call_time = NA,   excluded = NA,  total = NA,  perc_excluded = NA)
summary_set <- NULL
cumsum_exclude = 0

for(call_time  in 1:10){
  include <- nrow(cdr1[cdr1$CALL_ACTUAL_DURATION > call_time])
  exclude <- (total - include) / total *100
  excluded <- rbind(na.omit(excluded), data.frame(call_time, include, total, exclude))
  summary_set <- rbind(summary_set, broom::tidy(summary(cdr1$CALL_ACTUAL_DURATION[cdr1$CALL_ACTUAL_DURATION > call_time])))
}

par(mfrow=c(2,1))
plot(summary_set$median, ylab = "median call duration",  xlab = "minimal threshold of call duration (seconds)")
plot(excluded$exclude, ylab = "data excluded (%)", xlab = "minimal threshold of dall duration (seconds)")
par(mfrow = c(1,1))

# summary table of call duration excluding based on call duration from 1 to 10
summary_set
excluded
```

This table (above) give insights how much data will be thrown away when a certain threshold of minimal of call duration is applied.
* minimal call time above 5 seconds implies that 21,9 % of the original data is thrown away as invalid observations.
* minimal call time above 7 seconds implies that 25.3 % of the original data is thrown away as invalid observations.


```{r}
# drop calls under 5 seconds -> this implies wrong calls or perhaps voicemail calls
cdr1 <-
  filter(cdr1, CALL_ACTUAL_DURATION > 5)
```

#### Detect outliers based on calls (incoming or outgoing calls)

* During pre-processing of the CDR data, excluded the service numbers:
* one way to detect them is look at incoming calls, and exclude the +3 stan dev from the mean number of incoming calls

```{r}
# calculate number of incoming calls for a month per B_NUMBER
incoming_calls <-
  cdr1 %>%
  group_by(B_NUMBER) %>%
  summarise(nr = n()) %>%
  arrange(desc(nr))

# calculate number of incoming calls for a month per A_NUMBER
outgoing_call <-
  cdr1 %>%
  group_by(A_NUMBER) %>%
  summarise(nr = n()) %>%
  arrange(desc(nr))

# INCOMING CALL
# head(incoming_calls)
# # get summary stats
# summary(incoming_calls$nr)
# plot(density(incoming_calls$nr))
# plot(density(log(incoming_calls$nr)))
# 
# # get mean
# mean(incoming_calls$nr)
# # get standard dev
# sd(incoming_calls$nr)
# # calculate tripple sigma
# sigma3 <- ceiling(mean(incoming_calls$nr) + 3*sd(incoming_calls$nr))
# sigma
# # Q3 + interquartile *1.5
# outlier_quantile <- boxplot.stats(incoming_calls$nr)$out
# summary(outlier_quantile)
# # only 1 call made: valid or invalid?
# hist(incoming_calls$nr[incoming_calls$nr <= sigma3])
# hist(outlier_quantile)
# plot(x = 1:100, y = head(incoming_calls$nr, 100), type = "b")
# head(incoming_calls)
# # exclude 3671731
# 
# # OUTGOING CALLS
# plot(x = 1:100, y = head(outgoing_call$nr, 100), typ ="b")
# summary(outgoing_call$nr)
# summary(outgoing_call$unique_out)
# head(outgoing_call)

```

* Outgoing call: valid observations, order or max 40 made calls per month is still normal for highly usage.
* Incoming calls: one particual invalid observations of 637 incoming calls (in one month). That is really extreme. 
* Excluding this last number

#### Exclude outliers

exclude all customers that have either incoming-outgoings calls above 1400 calls a month.

```{r}


nrow(cdr1)
# 23932953
cdr1 <- 
  filter(cdr1, 
         !B_NUMBER %in% c(filter(incoming_calls, nr > 2000)$B_NUMBER, filter(outgoing_call, nr > 2000)$A_NUMBER), 
         !A_NUMBER %in% c(filter(incoming_calls, nr > 2000)$B_NUMBER, filter(outgoing_call, nr > 2000)$A_NUMBER)
  )

nrow(cdr1)
(1 - ( 24987280 / 25002277)) * 100

# 0.05 % thrown away
```

### Removing customers who call themself

```{r}
cdr1_breakdown <-
  filter(cdr1_breakdown, A_NUMBER != B_NUMBER)
```


## Creating breakdown structure

Next step is solving the date format, since the usage features will need a breakdown structure for calculations (based on days, houres, peak houres,..)


```{r}
# Variable manipulations
################################################################################################################

## CALL_START_DT to date format
## Create extra date/time variables like week-, daynumber, houres
cdr1_breakdown <-
  cdr1 %>% 
  mutate(
    CALL_START_DT = as.POSIXct(CALL_START_DT, "%d%b%Y", tz = "GMT"),
    CALL_START_DT_TM = as.POSIXct(paste(CALL_START_DT, CALL_START_TM),"%Y-%m-%d %H:%M:%S", tz = "GMT"),
    WEEK_NR = format(CALL_START_DT_TM, "%W"),
    DAY_NR = format(CALL_START_DT_TM, "%u"),
    DAY = format(CALL_START_DT_TM, "%a"),
    HOUR = as.integer(format(CALL_START_DT_TM, "%H")),
    PEAK_HOUR = ifelse(HOUR >= 9 & HOUR <= 19, "PEAK", "NON_PEAK"),
    WEEKDAYS = ifelse(DAY_NR %in% c(6,7), "WEEKEND", "WEEK")
  ) 

hist(cdr1_breakdown$HOUR)
abline(v = "9", col = "red")
abline(v = "19", col = "red")
```

First, explore our dataset and look for incomplete week sets.

* How many observations do we have on weekly bases
* conclusion: we should only take complete weeks into account

```{r}
# frequency
tidy(table(cdr1_breakdown$WEEK_NR)) %>% 
  ggplot(., aes(x = Var1, y = Freq/1000)) + geom_bar(stat = "identity") + ylab("number of observations (k)")
# first week 17 is not complete
```

```{r}
# Create a sequence interval of the time ordered data
# I choose grouping of 2 weeks
# week 18 - 19 => togehter in the same group nr 18
# week 20 - 21 => together in the same group nr 20
cdr1_breakdown <- 
  cdr1_breakdown %>% 
  mutate(
    mygroup = floor(as.numeric(WEEK_NR) / 2) *2) %>% # group 2 weeks in one
  group_by(mygroup) %>%
  # add max date of the 2 week intervalt
  mutate(
    max_date = max(CALL_START_DT)
  ) %>% ungroup 

```


## Get a sample of the data to continue with the overall process

```{r}
# take full set
sample_cdr <- cdr1_breakdown

# dim(cdr1_breakdown)
# take sample data
# do this with the smalles granularity, thus sample % on hour.

# take sample
# sample_cdr <- sample_frac(
#   group_by(cdr1_breakdown, HOUR), size = 0.05)
# check dimension
# dim(sample_cdr)

```

## Feature engineering

Main idea is to generate usage features for customers in the dataset. However, this needs to be done at regular intervals, taking a sequence static observations at multiple time points and using these intervals for feature extraction. I choose to do this per week, so change of behavior can be tracked by week. I will slice the data per week and create aggregated records of subscribers who where active during that week. So a customer who's active every week will contains +- 16 aggregated records (final dataset contains 4 months, each month +- 4 weeks).

I choose to do feature extraction for these features:

* nr_call = number of calls
* ave_call_time = average call time
* total_call_time = sum of all call times

#### Adding additional features

This steps generates features of the usage of a customer such as number of calls during the 2 week interval, but also more in detail like per day, per (non) peak hours, week or weekend days. Also, for the same breakdown structure the duration of calls is aggregated.


```{r}
# Variable creation based on usage
# create usage KPI 
################################################################################################################
# Create function to calculate the usage features
usage_stats <- function(dataset, group1, group2, ...) {
  # return a dataset with usage features
  dataset %>%
    group_by_(group1, group2, ...) %>%
    summarise(
      nr_call= n(),
      nr_unique_calls = length(unique(B_NUMBER)),
      ave_call_time = mean(CALL_ACTUAL_DURATION),
      total_call_time = sum(CALL_ACTUAL_DURATION)
    ) %>%
    gather_(key_col = "variable", value_col = "value", 
            gather_cols = c(
              "nr_call", 
              "nr_unique_calls",
              "ave_call_time", 
              "total_call_time")) %>%
    unite_("temp", c("variable", ...)) %>%
    spread_("temp", "value", fill = 0) %>%
    ungroup()
}

# number of outbound calls per group = 2 weeks 
# average outbound call duration per group
# total call time per group
week_set <-
  usage_stats(dataset = sample_cdr, "mygroup", "A_NUMBER")
# number of outbound calls per day over 2 weeks
# average outbound call duration per day over 2 weeks
# total call time per day over 2 weeks
# day_set <- 
# usage_stats(dataset = sample_cdr, "mygroup", "A_NUMBER", "DAY")
# number of outbound calls in (non) peak hours 
# average outbound call duration in (non) peak hours 
# total call time in (non) peak hours 
peak_set <-
  usage_stats(dataset = sample_cdr, "mygroup", "A_NUMBER", "PEAK_HOUR")
# number and % of outbound calls in week (end) days
# average outbound call duration per week (end) days
# total call time per week (end) days
weekday_set <- 
  usage_stats(dataset = sample_cdr, "mygroup", "A_NUMBER", "WEEKDAYS")
# number and % of outbound calls per target network
# average outbound call duration per target network
# total call time per per target network
network_set <- 
  usage_stats(dataset = sample_cdr, "mygroup", "A_NUMBER", "TARGET_AREA")

```


#### Add recency variable

Take into account the recency of call made within the interval of 2 weeks. This is implementen as 1 feature: the latest activity, but also incorporated into the social network analyses as edge weights. 

```{r}
# Calculate recency of calls for input in the decay function

decay_factor <- function(days, factor) {
  exp(1)^(-days * factor)
}

# First is almost linear,
# I prefer factor 0.4
par(mfcol=c(2,2))
plot(x = 0:13, y =  do.call(decay_factor, args = list(0:13, 0.1)), type = "b", xlab = "days", ylab = "recency", main = "exp(1)^(-days*0.1)")
plot(x = 0:13, y =  do.call(decay_factor, args = list(0:13 ,0.2)), type = "b", xlab = "days", ylab = "recency", main = "exp(1)^(-days*0.2)")
plot(x = 0:13, y =  do.call(decay_factor, args = list(0:13, 0.3)), type = "b", xlab = "days", ylab = "recency", main = "exp(1)^(-days*0.3)")
plot(x = 0:13, y =  do.call(decay_factor, args = list(0:13, 0.4)), type = "b", xlab = "days", ylab = "recency", main = "exp(1)^(-days*0.4)")

```

Calculate the recency or activity of each individual call
* used as edge weight in social network analysis

```{r}

# Add recency PER each call based on the 2 week interval
sample_cdr <-
  sample_cdr %>%
  mutate(
    diffday = as.integer(difftime(max_date,CALL_START_DT, units = "days")),
    weight = decay_factor(diffday, 0.3))
```

* used to calculate latest activity of a caller and added as customer variable

```{r}
# Calculate recency of last activity for a customer PER mygroup 
# get minimum of diffday for each customer
recency_set <- 
  sample_cdr %>%
  group_by(mygroup, A_NUMBER) %>%
  summarise(
    # diffday_min = min(diffday),
    recency_min = min(weight)) %>%
  ungroup %>% unique
```


Merge those sets to one dataset containing all the call variables.

```{r}

# merge dataset to a wide format
dataset_usage_kpi <- 
  Reduce(left_join, list(week_set, peak_set, weekday_set, network_set, recency_set)) 

rm(week_set, peak_set, weekday_set, network_set, recency_set)

```

## Final dataset with the usage features

Add percentages to all the call nr and call time variables

```{r}

# add percentage features for number of call per X and total call time per X
dataset_usage_kpi <-
  dataset_usage_kpi %>%
  mutate_at(vars(starts_with("nr_call_")), funs("perc" = . / nr_call)) %>%
  mutate_at(vars(starts_with("total_call_time_")), funs("perc" = . / total_call_time)) 
# %>%
# mutate_at(vars(starts_with("nr_unique_")), funs("perc" = . / nr_unique))

# print head full dataset with usage features
head(dataset_usage_kpi)
dim(dataset_usage_kpi)

```

## Add RFM variables

```{r}

dataset_usage_kpi <- 
  dataset_usage_kpi %>%
  group_by(mygroup) %>%
  mutate(global_nr_calls = sum(nr_call),
         global_time_call = sum(total_call_time),
         frequency_perc = nr_call / global_nr_calls,
         mon_value_perc = total_call_time / global_time_call) %>%
  select(-global_nr_calls, -global_time_call) %>%
  ungroup()


```



# Part 2: Social network Analysis

## Creating network
```{r, warning=FALSE}
library(doParallel)  
no_cores <- detectCores() - 1  
cl <- makeCluster(no_cores, type="FORK")  
# registerDoParallel(cl)  # only when using foreach

graphs_per_group <- 
  parLapply(cl, 
            split(
              select(sample_cdr, A_NUMBER, B_NUMBER, mygroup),
              sample_cdr$mygroup),
            graph_from_data_frame,  vertices = NULL, directed = TRUE )
stopCluster(cl)

# graphs_per_group <- 
#   lapply(
#     split(
#       select(sample_cdr, A_NUMBER, B_NUMBER, mygroup),
#       sample_cdr$mygroup), 
#     graph_from_data_frame, vertices = NULL,
#     directed = TRUE
#   )

summary(graphs_per_group)

# Creating unique list of vertices 
vertices <-
  unique(select(sample_cdr, mygroup, A_NUMBER))
vertices$mygroup <- as.character(vertices$mygroup)
```

## Example plot and network features
+ Decompose the graphs into interconnected graphs
In order to detect noise clusters, we iterate over all clusters and determine their sizes.
Clusters with less than a minimum amount of nodes will have all their nodes excluded. But before this, we need to make sure we don't exclude high profile customer? do we need this

```{r}
g <- simplify(
  graph.compose(
    graph.ring(10),
    graph.star(5, mode = "undirected")
  )
) + edge("7", "8") + edge("2", "3")

plot(g)
clusters(g)
g_decom <- decompose(g)

# no impact from unconnected graphs 
betweenness(g)
lapply(g_decom, betweenness)

# impact from unconnected graphs
closeness(g)
lapply(g_decom, closeness)

# no impact
degree(g)
lapply(g_decom, degree)

# impact
page_rank(g)$vector
lapply(g_decom, page_rank) %>% lapply(., function(x) x$vector)

# no impact
count_triangles(g)
lapply(g_decom, count_triangles)

component_distribution(g, mul.size = TRUE)
is_connected(g)
components(g, mode = "strong")
# ----------------------------------------------------------------------

# # Look if network is fully connected
# network_connected <- lapply(graphs_per_group, is_connected)
# 
# # look at size of each cluster in the network
# # For directed graphs, mode=STRONG 
# # calculates the strongly connected components, mode=WEAK calculates the weakly 
# # connected components (by practically ignoring the edge directions)
cluster_network <-
  lapply(graphs_per_group, components, mode = "weak")

# Which A_NUMBERS are in those clusters
cluster_membership <-
  lapply(cluster_network, function(x) x$membership) %>%
  lapply(as.data.frame) %>%
  lapply(., setNames, nm = "membership") %>%
  lapply(., rownames_to_column,  var = "NUMBER") %>%
  bind_rows(.id = "mygroup") %>%
  group_by(mygroup, membership) %>%
  mutate(csize = n()) %>%
  ungroup()

# summarise cluster size to see how big each cluster is, meaning how many clusters of a size x are there
cluster_size_sum <-
  cluster_membership %>%
  group_by(mygroup, csize) %>%
  summarise(nr_per_cluster = n_distinct(membership)) %>% 
  mutate(total_cust = nr_per_cluster * csize,
         cumsum_total_cust = cumsum(total_cust) / sum(total_cust))
```

Compairing the different subgraphs, for how much % do they represent the data.
```{r}
ggplot(cluster_size_sum, aes( x = log(csize), y = cumsum_total_cust)) +
  geom_point() + geom_line() + 
  # coord_cartesian(xlim = c(0, 10)) +
  # scale_x_continuous(breaks = seq(0,20,1), labels = seq(0,20,1)) +
  facet_wrap(~mygroup, scales = "free")
```


```{r}

# # Look at top of clusters and their size.
# # Enormous spike at cluster size. Lets check this out
top_n(cluster_size_sum, n = 1, wt = csize)

# Take all vertices of the highest connect subgraph
# Take this list to take the subset of customers to calculate the usage dataset
list_top_clusters <-
  cluster_membership %>% 
  select(mygroup, NUMBER,  csize) %>%
  group_by(mygroup) %>%
  filter(csize == max(csize)) %>%
  ungroup()


# TAKE LARGEST SUBGRAPH FUNCTION
giant.component <- function(graph, ...) {
  cl <- components(graph, ...)
  induced_subgraph(graph, which(cl$membership == which.max(cl$csize)))
  
}
# TAKE LARGEST SUBGRAPH
largest_graphs_per_group <- lapply(graphs_per_group, giant.component)

# Check dimension of graph
sapply(largest_graphs_per_group, gsize)

# Make A_NUMBER selection of the largest subgraph
# the NUMBERS of the subgraph are A_ & B_NUMBERS
# therefor, take a innerjoin per group to keep the A_NUMBERS in the largest subgraph

cdr1_breakdown$mygroup <- as.character(cdr1_breakdown$mygroup)

clear_sample <- 
  inner_join(list_top_clusters,cdr1_breakdown, by = c("mygroup" = "mygroup", "NUMBER" = "A_NUMBER"))



```

Are those high monetary value customers, meaning they represent a high volume or frequency of calls?

```{r}


# split largest network from the rest

gg <- induced_subgraph(graphs_per_group[['18']], vids = list_temp)


# check how many incoming edges it has
gg_deg_in_graphs <-
  degree(gg,  mode="in", loops = FALSE, normalized = FALSE) %>%
  as.data.frame() %>%
  rownames_to_column(., var = "B_NUMBER")
gg_deg_out_graphs <-
  degree(gg,  mode="out", loops = FALSE, normalized = FALSE) %>%
  as.data.frame() %>%
  rownames_to_column(., var = "A_NUMBER")

```



## Help function to convert network features into dataframe

todo: probeer simplify te gebruiken om multiple edges om te zetten naar weight

```{r}
return_dataset <- function(list_metrics, var_name)
{
  lapply(list_metrics, as.data.frame) %>%
    lapply(., select, 1) %>%
    lapply(., setNames, nm = var_name) %>% 
    lapply(., rownames_to_column,  var = "A_NUMBER") %>% 
    bind_rows(.id = "mygroup") %>%
    right_join(., vertices)
}

```

## Creating network features

Uitleg van verschillende centrality metrics

Foto's van definites toevoegen
plot degree distribution of network

#### Degree: just number of in & out
doest not factor in distance or weight


```{r}
# Degree (number of ties)
deg_all_graphs <-  
  return_dataset(
    lapply(graphs_per_group, degree, mode = "all", loops = FALSE, normalized = FALSE),
    "degree_all") 
deg_in_graphs <- 
  return_dataset(
    lapply(graphs_per_group, degree, mode="in", loops = FALSE, normalized = FALSE),
    "degree_in")
deg_out_graphs <- 
  return_dataset(
    lapply(graphs_per_group, degree, mode="out", loops = FALSE, normalized = FALSE),
    "degree_out")

# summary(deg_all_graphs)
# check ok

```

#### Strength or weighted vertex degree
* Summing up the edge weights of the adjacent edges for each vertex.
* better estimate for relevance of nodes since high number indicates more recent activities
* todo: normalize it

```{r}
# calculate the "weighted degree": Summing up the edge weights of the adjacent edges for each vertex.
# strenght_all <- lapply(graphs_per_group, strength, mode = "all", loops = FALSE) 
# strenght_in <- lapply(graphs_per_group, strength, mode = "in", loops = FALSE)
# strenght_out <- lapply(graphs_per_group, strength, mode = "out", loops = FALSE)
```


#### Closeness centrality of vertices

```{r}
# -> intensive computing
# closeness: shortest path / nodes, vanuit 1 enkele node naar andere
# farness/peripherality of a node v is defined as the sum of its distances to all other nodes
# Closeness (centrality based on distance to others in the graph)
# Inverse of the node’s average geodesic distance to others in the network.
# The more central a node is, the lower its total distance to all other nodes
net_closeness_in <-   
  return_dataset(
    lapply(graphs_per_group, estimate_closeness, mode = "in", cutoff = 8,  normalized = FALSE),
    "closeness_in")
net_closeness_out <- 
  return_dataset(
    lapply(graphs_per_group, estimate_closeness, mode = "out", cutoff = 8, normalized = FALSE),
    "closeness_out")


# closeness(graph = net, vids = V(net), mode = "all", normalized = TRUE)
# check ok
```


#### Transitivity of a graph

```{r}
# clustering coefficient.
# P(two randomly selected friends of A are friends)
# Transitivity measures the probability that the adjacent vertices of a vertex are connected. 
# This is sometimes also called the clustering coefficient. (0 = star, 1 clique)
take_transitivity <- function(graph, ...) {
  trans <- transitivity(graph, type = 'localundirected', isolates = 'zero')
  get_names <- V(graph)$name
  data.frame(A_NUMBER = get_names, transitivity = trans)  
}

net_transitivity <- 
  lapply(graphs_per_group, simplify) %>%
  lapply(., take_transitivity) %>%
  bind_rows(.id = "mygroup") %>%
  right_join(., vertices)

#check ok
```

#### Find triangles in graphs

```{r}
# Count how many triangles a vertex is part of, in a graph, or just list the triangles of a graph.
take_triangles <- function(graph, ...) {
  count_tri <- count_triangles(graph, ...)
  get_names <- V(graph)$name
  data.frame(A_NUMBER = get_names, triangles = count_tri)  
}
net_triangles <- 
  lapply(graphs_per_group, take_triangles) %>%
  bind_rows(.id = "mygroup") %>%
  right_join(., vertices)

# check ok
```

#### Vertex and edge betweenness centrality

```{r}
# -> very intensive computing

# Betweenness: defined by the number of geodesics (shortest paths) going through a vertex or an edge.
# Betweenness centrality quantifies the number of times a node acts as a bridge along the shortest path between two other nodes
# Betweenness (centrality based on a broker position connecting others)
# Number of geodesics that pass through the node or the edge.
# if to slow try estimate_betweeness
net_betweenness <- 
  return_dataset(
    lapply(graphs_per_group, simplify) %>%
      lapply(., estimate_betweenness, directed = TRUE, cutoff = 8),
    "betweenness")

# check ok
```


#### Pagerank

```{r}
# need different return_dataset function
# Google’s PageRank is a variant of the Eigenvector centrality measure for directed network

net_pagerank <- 
  lapply(graphs_per_group, page_rank, directed = TRUE, damping = 0.85) %>%
  map(., 1) %>%
  lapply(., as.data.frame) %>%
  lapply(., setNames, nm = "pagerank") %>% 
  lapply(., rownames_to_column,  var = "A_NUMBER") %>% 
  bind_rows(.id = "mygroup") %>%
  right_join(., vertices)


# check ok
```


# Putting it all together

```{r}

dataset_usage_kpi$mygroup <- as.character(dataset_usage_kpi$mygroup)
dataset_usage_kpi$A_NUMBER <- as.character(dataset_usage_kpi$A_NUMBER)

finalset <- 
  Reduce(left_join, 
         list(
           dataset_usage_kpi,
           deg_all_graphs, 
           deg_in_graphs, 
           deg_out_graphs,
           net_betweenness,
           net_closeness_in,
           net_closeness_out,
           net_pagerank,
           net_triangles,
           net_transitivity
         ))

# fill NA of transitivity or triangles with 0

finalset[is.na(finalset)] <- 0

rm(deg_all_graphs,deg_in_graphs, deg_out_graphs, net_betweenness, net_closeness_in,
   net_closeness_out, net_pagerank, net_triangles, net_transitivity)

```


# BACKUP STEP: finalset vanaf hier



# Part 3: Cluster

```{r}
# libraries

```



Make unique rownmames of mygroup + A_NUMBER
```{r}
# finalset <- sample_frac(finalset, size = 0.30, replace = FALSE )
# Make a rownames of the group + A-NUMBER
finalset <- 
  finalset %>%
  unite(col = key, mygroup, A_NUMBER, sep = "_", remove = FALSE)

class(finalset) <- "data.frame"
row.names(finalset) <- finalset$key
finalset <- finalset[,-c(2:3)]

# Remove irrelevant features
# keep everything, except home, other net
clusterset <- 
  dplyr::select(finalset,
                key,
                nr_unique_calls,
                recency_min,
                nr_call,
                total_call_time,
                ave_call_time,
                nr_call_NON_PEAK_perc,
                nr_call_PEAK_perc,
                nr_call_WEEK_perc,
                nr_call_WEEKEND_perc,
                total_call_time_NON_PEAK_perc,
                total_call_time_PEAK_perc,
                total_call_time_WEEK_perc,
                total_call_time_WEEKEND_perc,
                degree_all,
                degree_in,
                betweenness,
                closeness_in,
                closeness_out,
                pagerank,
                triangles,
                transitivity)

# TAke scaling: Range vs scaling (x-md/sd)
range01 <- function(x){(x-min(x))/(max(x)-min(x))}
### Range everything between value of [0-1]
clusterset_ranged <-
  clusterset %>%
  mutate_each(funs((.-min(.))/(max(.)-min(.))), -key)

### Scale 
clusterset_scaled <-
  clusterset %>%
  mutate_each(funs((. - mean(.)) / sd(.)), -key)

```

### Take samples
```{r}
clusterset <- sample_frac(clusterset, size = 0.2,replace = FALSE)
clusterset_ranged <- filter(clusterset_ranged, key %in% clusterset$key)
clusterset_scaled <- filter(clusterset_scaled, key %in% clusterset$key)

```




### Delete correlated features
```{r}

delete_correlation <- function(dataset, threshold, plotcor) {
  require(caret)
  # plot correlation plot
  cor_set <- cor(dataset[,-1])
  if(plotcor == TRUE) corrplot::corrplot(cor_set, tl.cex = 0.5, type = "lower", method = "number", number.cex = 0.5)
  # This function searches through a correlation matrix and returns a vector of integers corresponding to columns to remove to reduce pair-wise correlations.
  high_correlated = findCorrelation(cor_set, cutoff= threshold, exact = TRUE) # Correlation has to be higher than 75% #
  
  # remove higly correlated (must be irrelevant) 
  if(length(high_correlated) > 0){
    cor_set_return = dataset[,-c(high_correlated)]
  }
  
  return(cor_set_return)
} 

# clusterset with no correlated features
# set correlation threshold
clusterset_no_cor <- delete_correlation(clusterset_scaled, 0.8, FALSE)
clusterset2_no_cor <- delete_correlation(clusterset2_scaled, 0.8, FALSE) # no perc
clusterset3_no_cor <- delete_correlation(clusterset3_scaled, 0.8, FALSE)

```


### Select only usage features

```{r}
# clusterset with only usage features

# clusterset, scaled, percentage features + network features
clusterset_usage_scaled <- 
  select(clusterset_scaled, -degree_all, -degree_in, -c(betweenness:transitivity) )
# clusterset, NOT scaled, percentage features + network features
clusterset_usage <-
  select(clusterset, -degree_all, -degree_in, -c(betweenness:transitivity) )

# clusterset, scaled, NO percentage features
clusterset2_usage_scaled <-
  select(clusterset2_scaled, -degree_all, -degree_in, -c(betweenness:transitivity) )

# clusterset, NOT scaled, NO percentage features 
clusterset2_usage <-
  select(clusterset2, -degree_all, -degree_in, -c(betweenness:transitivity) )


```

# Cluster

## K-mean attempt

Do this several times with incremented number of clusters.
* Evaluatie by looking at the SSE , this is called the elbow technique

```{r}
kmean_fun <- function(dataset, max_cluster) {
  require(foreach)
  require(doParallel)
  no_cores <- detectCores()-1
  #nr of processors available registerDoParallel(cl)
  cl<-makeCluster(no_cores, type = "FORK") #nr of processors available 
  registerDoParallel(cl)
  
  # result.kmean <- (nrow(dataset)-1)*sum(apply(dataset,2,var))
  result.kmean <- foreach(i=1:max_cluster) %dopar% {
    km <- kmeans(dataset, centers = i, iter.max = 30, nstart = 10);
    return(km)
  }
  stopImplicitCluster()
  stopCluster(cl)
  return(result.kmean)
}


kmean_ranged <- kmean_fun(clusterset_ranged[,-1], 15)
kmean_scaled <- kmean_fun(clusterset_scaled[,-1], 15)

```

## Kmeans-run (detect optimal clusters)
Try other function: fpc::kmeansruns
* this will also incremented the number of cluster in each run and eavluate it automatically with the  Calinski Harabasz index

```{r}
# 

kmeanruns_fun <- function(dataset, max_cluster) {
  require(fpc)
  # require(foreach)
  # require(doParallel)
  # no_cores <- detectCores()-1
  # #nr of processors available registerDoParallel(cl)
  # cl<-makeCluster(no_cores, type = "FORK") #nr of processors available 
  # registerDoParallel(cl)
  
  # result.kmean <- (nrow(dataset)-1)*sum(apply(dataset,2,var))
  km <- kmeansruns(dataset, krange= 1:15, criterion = "ch", runs = 10, iter.max = 30);
  return(km)
  # 
  # stopImplicitCluster()
  # stopCluster(cl)
  # return(result.kmean)
}

kmeansrun_ranged <- kmeanruns_fun(clusterset_ranged[,-1])
kmeanruns_scaled <- kmeanruns_fun(clusterset_scaled[,-1])


```

### Evaluate K-mean clusters

I have a preference for clusterset 3 with 10 clusters.

#### Elbow technique
Plot the withiness sum of squared distance
```{r}
elbow_plot <- function(dataset, plotname) {
  plot(1:15, sapply(dataset, function(x) x$tot.withinss), type="b", xlab="Number of Clusters", ylab="Within groups sum of squares", main = plotname)
}
png("elbow_kmeans_final.png", width = 500,  height = 800, res = 100)
par(mfrow=c(2,1))
elbow_plot(kmean_scaled, "Clusterset ranged")
elbow_plot(kmean_ranged, "Clusterset scaled")
dev.off()
```

### Calinski-Harabsz 

The outcome of the CH measure is a number between 0 and + infinity, a larger number indicates a better cluster quality 
```{r}
ch_plot <- function(dataset, datacluster, plotname) {
  require(clusterSim)
  par(mfrow=c(1,2))
  plot(1:15, sapply(datacluster, function(i) index.G1(dataset[,-1], cl = i$cluster)), type="b", xlab="Number of Clusters", ylab="Davies-Bouldin [0,+oneindig] smaller = better", main = plotname)
  plot(1:15, sapply(datacluster, function(i) index.DB(dataset[,-1], cl = i$cluster)$DB), type="b", xlab="Number of Clusters", ylab="Davies-Bouldin [0,+oneindig] smaller = better", main = plotname)
}

ch_plot(clusterset_ranged, kmean_ranged, "Clusterset 3 scaled, perc, manual select")
ch_plot(clusterset_scaled, kmean_scaled, "Clusterset 4 scaled, perc, manual select")


# check fit of clusters
measure_cluster <- function(dataset, datacluster) {
  ####STATISTICS
  require(clusterSim)
  indexCH <- index.G1(dataset[,-1], datacluster)
  indexDB <- index.DB(dataset[,-1], datacluster) 
  cat('Davies-Bouldin [0,+oneindig] smaller = better: ',indexDB$DB,'\nDavies-Bouldin [0,+oneindig] smaller = better: ',indexCH) 
  list(CH = indexCH, DB = indexDB$DB)
}
measure_cluster(clusterset_ranged, kmean_ranged[[10]]$cluster)
measure_cluster(clusterset_scaled, kmean_scaled[[10]]$cluster)

```

## K-Medoid

PAM (Partitioning Around Medoids) is a classic algorithm for k-medoids clustering. While the PAM algorithm is inefficient for clustering large data, the CLARA algorithm is an enhanced technique of PAM by drawing multiple samples of data, applying PAM on each sample and then returning the best clustering. It performs better than PAM on larger data. Functions pam() and clara() in package cluster [Maechler et al., 2015] are respectively im- plementations of PAM and CLARA in R. For both algorithms, a user has to specify k, the number of clusters to find. As an enhanced version of pam(), function pamk() in package fpc [Hennig, 2015] does not require a user to choose k. Instead, it calls the function pam() or clara() to perform a partitioning around medoids clustering with the number of clusters estimated by optimum average silhouette width.

```{r}

pamk_fun <- function(dataset, iterations) {
  # no_cores <- detectCores()-1
  #nr of processors available registerDoParallel(cl)
  # cl<-makeCluster(no_cores) #nr of processors available 
  # registerDoParallel(cl)
  
  # result.pamk <- (nrow(dataset)-1)*sum(apply(dataset,2,var))
  # result.pamk <- foreach(i=1:iterations) %dopar% {
  km <- pamk(dataset, krange = 1:15,  criterion="ch", usepam = FALSE);
  return(km)
  # }
  # stopImplicitCluster()
  # stopCluster(cl)
  # return(result.pamk)
}

pamk_ranged <- pamk_fun(clusterset_ranged[,-1])
pamk_scaled <- pamk_fun(clusterset_scaled[,-1])

```

## DBSCAN
```{r}
library(dbscan)
library(fpc)

distplot_clusterset_scaled <- kNNdistplot(clusterset_scaled[,-1], k =  5)
distplot_clusterset_scaled

abline(h = 0.21, lty = 2)

dbscan_clusterset_scaled <- dbscan(x = clusterset3_scaled[,-1], eps = 0.2, borderPoints = TRUE)
```


# Describe cluster

Look at cluster characteristics of the scaled clusterset with no pertages
```{r}

cluster_analyse <- function(dataset, clustervector, fun) {
  # characteristics of each cluster
  charact <- 
    aggregate(clusterset[,-1],  by = list(cluster = pamk_ranged$pamobject$clustering), mean)
  print(charact)
  
  # put it in a long format to plot with ggplot
  charact_long <-
    charact %>%
    gather(vars, value, -cluster) %>%
    group_by(vars) %>%
    mutate(value = range01(value))  
  # ggplot heatmap with profile of each cluster
  
  orderit <- c("nr_unique_calls","recency_min", "nr_call", "total_call_time", "ave_call_time", "nr_call_NON_PEAK_perc", "nr_call_PEAK_perc", "nr_call_WEEK_perc", "nr_call_WEEKEND_perc", "total_call_time_NON_PEAK_perc", "total_call_time_PEAK_perc", "total_call_time_WEEK_perc", "total_call_time_WEEKEND_perc" ,"degree_all", "degree_in", "betweenness", "closeness_in", "closeness_out", "pagerank", "triangles", "transitivity")
  
  charact_long$vars <- factor(charact_long$vars, levels = orderit)
  
  ggplot(charact_long, aes(x = as.factor(cluster), y = vars)) + geom_tile(aes(fill = value)) + theme_minimal()+ scale_fill_gradient(low = "white", high = "steelblue") + theme_bw() + theme(axis.ticks = element_blank(), panel.border = element_blank()) + scale_x_discrete(expand = c(0, 0))
  ggsave(filename =  deparse(substitute(clustervector)), device = "png")
  
}




```

